# Ethereum Log Parser

## Acknowledgment

This project is based on [ethereum-scraping](https://github.com/bhemen/ethereum-scraping/) repo. Credit goes to [bhemen](https://github.com/bhemen).

## Abstract

This project is designed to efficiently parse Ethereum logs into events using the ABI (Application Binary Interface) of the contract being scrapped. The primary source of these logs is [Google BigQuery](https://cloud.google.com/bigquery), thanks to its partnership with [Nansen](https://www.nansen.ai/), allowing for rapid retrieval of logs associated with specific smart contracts.

## Why Google BigQuery?

While public nodes like Infura and Alchemy offer access to Ethereum logs, they might require subscriptions for extensive data queries and can be rate-limited. Running a personal Ethereum node, although free of subscription costs, is bound by hardware limitations and might not offer the same speed as commercial services.

Google BigQuery, in collaboration with [Nansen](https://docs.nansen.ai/database-access/getting-started-database), provides a swift and efficient way to access all Ethereum logs. Using just the smart contract address, you can download all associated logs within a matter of minutes or seconds, ready to be parsed locally. Most importantly, BigQuery credit each new user $300, which is more than enough for casual usage.

## Repository Structure

The project consists of two main scripts:
1. [preprocess_jsonlogs.py](scripts/preprocess_jsonlogs.py)

    - Consolidates multiple JSON logs generated by BigQuery into a single DataFrame.
    - Extracts event names from the logs using the contract's ABI, print event counts, then
    save logs to csv.

2. [parse_allevents.py](scripts/parse_allevents.py)

    - Processes the DataFrame/csv generated by [preprocess_jsonlogs.py](scripts/preprocess_jsonlogs.py), with each row representing an individual log.
    - Categorizes and decodes logs according to the contract's ABI.
    - Outputs each distinct event type into separate CSV files (e.g., Approval, Burn, Transfer events for ERC20 contracts).

## How to Use

1. Google BigQuery Setup:

    - Register for a free account with Google BigQuery.
    Run the following SQL query to get logs for a specific contract:
    ```sql
    SELECT * FROM `bigquery-public-data.crypto_ethereum.logs` WHERE UPPER('contract_address_here') = UPPER("{address}");
    ```


    - Once the query is complete, a result table will appear. Click "Export", choose "Export to GCS".
    - Choose a Google Cloud Storage bucket and specify the filename with a wildcard (e.g., filename_*). Make sure to select the JSON format.
    - The entire table will be exported to your Google Cloud Storage bucket in multiple files (size dependent).

2. Downloading Data from Google Cloud Storage:

    - It's recommended to use the gcloud command-line tool to download files from the bucket:

    ```bash
    gcloud storage cp gs://BUCKET_NAME/OBJECT_NAME SAVE_TO_LOCATION
    ```

    - Detailed instructions for downloading objects from Google Cloud Storage can be found [here](https://cloud.google.com/storage/docs/downloading-objects).

3. Using the Scripts:

    Once you've downloaded the data from Google Cloud Storage, you can proceed to use the scripts [preprocess_jsonlogs.py](scripts/preprocess_jsonlogs.py) and [parse_allevents.py](scripts/parse_allevents.py) as detailed above.

    - Set up the Python venv from [requirements.txt](requirements.txt)

    - Specify the folder path in which the logs are stored and contract name. Run [preprocess_jsonlogs.py](scripts/preprocess_jsonlogs.py), then run [parse_allevents.py](scripts/parse_allevents.py).
    - Note that ```get_cached_abi``` in [preprocess_jsonlogs.py](scripts/preprocess_jsonlogs.py) will try to get and set the abis from Etherscan, yet it does not work with proxy address. First run of ```get_cached_abi``` will generate ```abis/cached_abis.json```. Manually overwrite the file (keep the filename) with the correct abis will solve the problem.